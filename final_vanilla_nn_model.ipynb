{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping columns that are dominated by the null values\n",
    "df.drop(['ps_reg_03','ps_car_03_cat', 'ps_car_05_cat'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#float cols used later, kept here so operation won't change the dtype of cols\n",
    "#update this if any float cols are deleted\n",
    "float_cols = df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "print('Number of float columns: ', len(float_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#storing rows with null values in sep. dataframe for later use\n",
    "df_any_null = df[(df == -1).any(axis = 1)]\n",
    "print('Number of rows with atleast one null value: ', len(df_any_null))\n",
    "\n",
    "#df with no null values.\n",
    "df = df[~(df == -1).any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping binary columns that are dominated by a single level\n",
    "df.drop(['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_13_bin'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping categorical columns that are dominated by a single level\n",
    "df.drop(['ps_car_10_cat'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping a column that has correlation with another column, ps_ind_14\n",
    "df.drop(['ps_ind_12_bin'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cat. column names\n",
    "col_names = [col for col in df.columns if '_cat' in col]\n",
    "\n",
    "#creating dummy variables for categorical variables \n",
    "b = []\n",
    "for col in col_names:\n",
    "    if len(df[col].unique()) > 2:\n",
    "        b.append(col)\n",
    "#dummies, automatically drops parent column        \n",
    "df = pd.get_dummies(df,columns=b,prefix=b)\n",
    "\n",
    "del(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalizing the float columns\n",
    "#dict to save the min, mean and max of a float column. to be used on test file\n",
    "fcols_summ = {}\n",
    "for col in float_cols:\n",
    "    t_d = {}\n",
    "    t_d['mean'] = df[col].mean()\n",
    "    t_d['max'] = df[col].max()\n",
    "    t_d['min'] = df[col].min()\n",
    "    range_ = t_d['max'] - t_d['min']\n",
    "    df[col] = (df[col] - t_d['mean'])/range_\n",
    "    fcols_summ[col] = t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rearranging cols for convolutions\n",
    "ind_cols = []\n",
    "reg_cols = []\n",
    "car_cols = []\n",
    "calc_cols = []\n",
    "for col in df.columns:\n",
    "    if 'ind' in col:\n",
    "        ind_cols.append(col)\n",
    "    if 'reg' in col:\n",
    "        reg_cols.append(col)\n",
    "    if 'car' in col:\n",
    "        car_cols.append(col)\n",
    "    if 'calc' in col:\n",
    "        calc_cols.append(col)\n",
    "        \n",
    "cols_order = ind_cols+reg_cols+car_cols+calc_cols\n",
    "print('No of Columns: ', len(cols_order))\n",
    "df = df[['id', 'target'] + cols_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting data into training and test sets\n",
    "#shuffle for train_test_split by default is true\n",
    "training_features, test_features, \\\n",
    "training_target, test_target, = train_test_split(df.drop(['id','target'], axis=1),\n",
    "                                               df['target'],\n",
    "                                               test_size = .2,\n",
    "                                               random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting train data to train and validation sets\n",
    "#shuffle for train_test_split by default is true\n",
    "x_train_res, x_val, y_train_res, y_val = train_test_split(training_features, training_target,\n",
    "                                                  test_size = .1,\n",
    "                                                  random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SMOTE, as the dataset is imbalanced\n",
    "sm = SMOTE(random_state=12, ratio = 'minority', kind='borderline1')\n",
    "x_train, y_train = sm.fit_sample(x_train_res, y_train_res)\n",
    "\n",
    "assert(len(y_train) == np.shape(x_train)[0])\n",
    "\n",
    "print('After SMOTE x type: ', type(x_train))\n",
    "print('After SMOTE x shape: ', np.shape(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Smote is for continuous variables but as we have both cont. and categorical var. we should use SMOTE-NC as described in the \n",
    "following papper: https://www.jair.org/media/953/live-953-2037-jair.pdf\n",
    "\n",
    "Alternatively, I chose to round off the categorical variables to the nearest integer.\n",
    "'''\n",
    "cols = df.drop(['id','target'], axis=1).columns\n",
    "\n",
    "#cat_idx is list of index position of cols that are categorical\n",
    "cat_idx = []\n",
    "i = 0\n",
    "for col in cols: \n",
    "    if ('_bin' in col) or ('_cat' in col):\n",
    "        cat_idx.append(i)\n",
    "    i = i + 1\n",
    "\n",
    "#in numpy axis = 0 implies a column\n",
    "x_train[:, cat_idx] = np.apply_along_axis(np.round, axis = 0, arr=x_train[:, cat_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of training examples\n",
    "n_train = len(y_train)\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = len(y_val)\n",
    "\n",
    "# Number of testing examples.\n",
    "#n_test = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "batch_size = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = x_val.as_matrix()\n",
    "y_val = y_val.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(90, 200)\n"
     ]
    }
   ],
   "source": [
    "print('type of trainging set',type(x_train))\n",
    "print('Initial shape of training set: ',np.shape(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshaping the array to be given as input to 3D\n",
    "\n",
    "x_train = x_train[:,np.newaxis,:,np.newaxis]\n",
    "print('New shape of training set: ',np.shape(x_train))\n",
    "\n",
    "x_val = x_val[:,np.newaxis,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None,1,200,1])\n",
    "y_ = tf.placeholder(tf.int32, [None])\n",
    "y = tf.one_hot(y_, 2)\n",
    "\n",
    "l = tf.placeholder(tf.float32) #learning rate placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_(x, wts, bias, stride=1, padding='VALID'):\n",
    "    x = tf.nn.conv2d(x, wts, [1,1,stride,1], padding)\n",
    "    x = tf.nn.bias_add(x, bias)\n",
    "    return tf.nn.relu(x)  \n",
    "\n",
    "def NN_lay(x, wts, bias):\n",
    "    x = tf.add(tf.matmul(x, wts), bias)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first conv layer variables\n",
    "cnw_1 = tf.Variable(tf.truncated_normal([1,3,1,8], mean=0, stddev=0.1)) #stride 1\n",
    "cnb_1 = tf.Variable(tf.zeros([8]))\n",
    "\n",
    "#second conv layer variables\n",
    "cnw_2 = tf.Variable(tf.truncated_normal([1,3,8,16], mean=0, stddev=0.1)) #stride 2\n",
    "cnb_2 = tf.Variable(tf.zeros([16]))\n",
    "\n",
    "#third conv layer variables\n",
    "cnw_3 = tf.Variable(tf.truncated_normal([1,3,16,24], mean=0, stddev=0.1)) #stride 3\n",
    "cnb_3 = tf.Variable(tf.zeros([24]))\n",
    "\n",
    "#NN wts and bias\n",
    "nnwts_1 = tf.Variable(tf.truncated_normal([96, 128], mean=0, stddev=0.1))\n",
    "nnb_1 = tf.Variable(tf.zeros([128]))\n",
    "\n",
    "nnwts_2 = tf.Variable(tf.truncated_normal([128, 64], mean=0, stddev=0.1))\n",
    "nnb_2 = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "nnwts_3 = tf.Variable(tf.truncated_normal([64, 32], mean=0, stddev=0.1))\n",
    "nnb_3 = tf.Variable(tf.zeros([32]))\n",
    "\n",
    "nnwts_4 = tf.Variable(tf.truncated_normal([32, 2], mean=0, stddev=0.1))\n",
    "nnb_4 = tf.Variable(tf.zeros([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = conv_(x, cnw_1, cnb_1)\n",
    "#c1 = tf.shape(logits)\n",
    "logits = tf.nn.max_pool(logits, [1,1,2,1], [1,1,2,1], 'VALID')\n",
    "#m1 = tf.shape(logits)\n",
    "\n",
    "logits = conv_(logits, cnw_2, cnb_2, stride=2,padding='VALID')\n",
    "#c2 = tf.shape(logits)\n",
    "#padding logits\n",
    "logits = tf.pad(logits, tf.convert_to_tensor([[0,0],[0,0],[1,1],[0,0]]))\n",
    "#after_pad = tf.shape(logits)\n",
    "logits = tf.nn.max_pool(logits, [1,1,3,1], [1,1,3,1], 'VALID')\n",
    "#m2 = tf.shape(logits)\n",
    "\n",
    "logits = conv_(logits, cnw_3, cnb_3,stride=2,padding='VALID')\n",
    "#c3 = tf.shape(logits)\n",
    "logits = tf.nn.max_pool(logits, [1,1,2,1], [1,1,2,1], 'VALID')\n",
    "#m3 = tf.shape(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = flatten(logits)\n",
    "\n",
    "logits = NN_lay(logits, nnwts_1, nnb_1)\n",
    "\n",
    "logits = NN_lay(logits, nnwts_2, nnb_2)\n",
    "\n",
    "logits = NN_lay(logits, nnwts_3, nnb_3)\n",
    "\n",
    "logits = tf.add(tf.matmul(logits, nnwts_4), nnb_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy loss is objective function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# Optimizer \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=l).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec = tf.metrics.recall(y, tf.round(tf.nn.softmax(logits)))\n",
    "prec = tf.metrics.precision(y, tf.round(tf.nn.softmax(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf. global_variables_initializer()\n",
    "\n",
    "#saving the model\n",
    "save_file = 'New_wts/model'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    no_of_batches = int(len(y_train)/batch_size)\n",
    "    print('No of batches: ', no_of_batches)\n",
    "    x_t, y_t = shuffle(x_train, y_train)\n",
    "    for epoch in range(epochs):\n",
    "        strt_tym = time.time()\n",
    "        for offset in range(no_of_batches):\n",
    "            idx = np.random.randint(0, high=len(y_train), size=batch_size)\n",
    "            batch_x, batch_y = x_train[idx], y_train[idx]\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={x:batch_x, y_:batch_y, l:learning_rate})\n",
    "            loss = sess.run(cost, feed_dict={x:batch_x, y_:batch_y})\n",
    "            \n",
    "            if offset//10 == 0:\n",
    "                print('------------------------------------------------')\n",
    "                print('Offset No: ', offset)\n",
    "                print('Precision on Validation Set: ', sess.run(prec, feed_dict={x: x_val, y_: y_val}))\n",
    "                print('Recall on Validation Set: ', sess.run(rec, feed_dict={x: x_val, y_: y_val}))\n",
    "    saver.save(sess, save_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1= time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2= time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.719599962234497"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
