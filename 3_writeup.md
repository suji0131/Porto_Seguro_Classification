# Safe Driver Prediction - Porto Seguro ([Kaggle Competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction))

## Problem Description & Data Summary
Built a model (Wide & Deep Classifier) that predicts the probability a driver will initiate an auto insurance claim in the next year. Each data point has ten continuous features, fourteen categorical features, seventeen binary variables and sixteen ordinal features. All features are anonymized and are given generic names. Target variable indicates whether the driver initiated a claim or not. So, this is a binary classification problem. But they are only interested in probablities, not class, indicated by the evaluation metric they chose, Normalized Gini Coefficient (NGC). NGC has a range of 0 (random guessing) to 1. There are 595,212 samples in the training set. Null values in the dataset are represented by -1.

## Data Preprocessing
As you can see from [1_Data_Pre_Processing](https://github.com/suji0131/Porto_Seguro_Classification/blob/master/1_Data_Pre_Processing.ipynb) file and image below the dataset is highly unbalanced. To address this, I initially used approaches like Synthetic Minority Over-sampling Technique (SMOTE) and Undersampling to balance the dataset and using balanced data to train classifier (decision trees, neural network, cnn(1-d convolutions) etc). These approches didn't yield the results I expected. I also built a Autoencoder but it also didn't improve the Gini coefficent. Explanation on why I finally chose the wide and deep network will be discussed later.

![Data Distribution](https://github.com/suji0131/Porto_Seguro_Classification/blob/master/Pre_processing_plots/Step%203.png)

Some of the columns are completely dominated by null values (> 95%) which are dropped. And normality test on continuous and ordinal columns showed that they don't follow normal distribution. So, null values for those  columns are replaced by median values instead of mean values. There is no significant correlation and multicorrelation between the features (which is why CNN are a bad idea for this problem). For discreet variables, distribution of classes is studied and features with a single dominating class are removed from dataset. Using Random Forests (optimal parameters are found using grid search) I mapped feature importance, shown below. I dropped features with low importance and trained wide & deep model with subset of important features but results were worse than using whole data. 

![Feature Importance](https://github.com/suji0131/Porto_Seguro_Classification/blob/master/Pre_processing_plots/Feature%20Importance%20Plot.png)

## Model Architecture
Here is an excellent [article](https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) on wide & deep model from people who developed the model and here is their [research paper](https://arxiv.org/abs/1606.07792). This model is developed as app recommendation system for android playstore. There are similarities with data we are working on and their data, like sparse feature vectors and over generalization resulting from unbalanced data. Out of all classifiers I tried this one gave me best normalized gini coefficient, 0.281 (competition winner has a score of 0.297).
